# Reporte: Singular Value Decomposition on GPU using CUDA.

La factorización SVD es una técnica importante utilizada para la factorización de una matriz rectangular. Bajo SVD las
operaciones con matrices son más robustas por lo que se utiliza para calcular la pesudo-inversa de una matriz, resolver sistemas de ecuaciones lineales u optimización de mínimos cuadrados, entre otros.

En principio la factorización SVD de una matriz **A** está definida por:
<a href="http://www.codecogs.com/eqnedit.php?latex=A=U&space;\Sigma&space;V^T" target="_blank"><img src="http://latex.codecogs.com/gif.latex?A=U&space;\Sigma&space;V^T" title="A=U \Sigma V^T" /></a>
donde <a href="http://www.codecogs.com/eqnedit.php?latex=U&space;\in&space;\mathbb{R}^{mxn},&space;V&space;\in&space;\mathbb{R}^{nxn},&space;\Sigma&space;\in&space;\mathbb{R}^{mxn}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?U&space;\in&space;\mathbb{R}^{mxn},&space;V&space;\in&space;\mathbb{R}^{nxn},&space;\Sigma&space;\in&space;\mathbb{R}^{mxn}" title="U \in \mathbb{R}^{mxn}, V \in \mathbb{R}^{nxn}, \Sigma \in \mathbb{R}^{mxn}" /></a> matrices
ortognales y <a href="http://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a> matriz diagonal con <a href="http://www.codecogs.com/eqnedit.php?latex=s_{ii}>=0" target="_blank"><img src="http://latex.codecogs.com/gif.latex?s_{ii}>=0" title="s_{ii}>=0" /></a> con orden descendiente.

El objetivo del artículo _Singular Value Decomposotion on GPU using CUDA_ es realizar esta factorización en la GPU aprovechando el increíble poder de cómputo que la caracteriza. El rápido crecimiento en desempeño del hardware de gráficos llevó a la GPU a ser un fuerte candidato para realizar múltiples operaciones de caracter intensivo y en paralelo. Es por esto que han surgido diversos ambientes que soporten la programación en GPU, NVIDIA provee CUDA como un ambiente operacional
con un lenguaje tipo C para los procesadores programables.

Este artículo expone que aunque el cómputo en la GPU se ha aprovechado para el cómputo científico no lo ha sido para la factorización SVD, problema que por sus múltiples aplicaciones se ve necesario realizar dicha implementación.

La factorización SVD se puede obtener por diversos algortimos, en este artículo se expone el de Golub-Reinsch (Bidiagonalización y diagonalización) dado que es simple y compacto además de mapear la arquitectura SIMD a la GPU. En resumen este algortimo realiza lo siguiente:

  1. La matriz A se reduce a una matriz bidiagonal usando una serie de transformaciones HouseHolder.
  2. La matriz reducida se diagonaliza utilizando iteraciones QR alternadas implícitamente.

En notación matemática se resume de la siguiente manera:

  1. <a href="http://www.codecogs.com/eqnedit.php?latex=B&space;\leftarrow&space;Q^TAP" target="_blank"><img src="http://latex.codecogs.com/gif.latex?B&space;\leftarrow&space;Q^TAP" title="B \leftarrow Q^TAP" /></a> {Bidiagonalización de A a B}
  2. <a href="http://www.codecogs.com/eqnedit.php?latex=\Sigma&space;\leftarrow&space;X^TBY" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\Sigma&space;\leftarrow&space;X^TBY" title="\Sigma \leftarrow X^TBY" /></a> {Diagonalización de B a <a href="http://www.codecogs.com/eqnedit.php?latex=\Sigma" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\Sigma" title="\Sigma" /></a>}
  3. <a href="http://www.codecogs.com/eqnedit.php?latex=U&space;\leftarrow&space;QX" target="_blank"><img src="http://latex.codecogs.com/gif.latex?U&space;\leftarrow&space;QX" title="U \leftarrow QX" /></a>
  4. <a href="http://www.codecogs.com/eqnedit.php?latex=V^T&space;\leftarrow&space;(PY)^T" target="_blank"><img src="http://latex.codecogs.com/gif.latex?V^T&space;\leftarrow&space;(PY)^T" title="V^T \leftarrow (PY)^T" /></a> {Calcular matrices U,V^T ortogonales y SVD de <a href="http://www.codecogs.com/eqnedit.php?latex=A=U&space;\Sigma&space;V^T" target="_blank"><img src="http://latex.codecogs.com/gif.latex?A=U&space;\Sigma&space;V^T" title="A=U \Sigma V^T" /></a>}

Es aquí cuando comienza lo interesante, debemos encontrar un algoritmo para poder realizar la bidiagonalización de A a B; para este caso en particular se realiza una serie de transformaciones Householder unitarias que nos ayudarán a obtener una matriz con ceros en la i-ésima fila e i-ésima columna, por ejemplo para la primera iteración: Para una matriz A de mxn, tomamos <a href="http://www.codecogs.com/eqnedit.php?latex=u^{(1)}\in\mathbb{R}^m" target="_blank"><img src="http://latex.codecogs.com/gif.latex?u^{(1)}\in\mathbb{R}^m" title="u^{(1)}\in\mathbb{R}^m" /></a> para el vector A(1:m,1) y <a href="http://www.codecogs.com/eqnedit.php?latex=v^{(1)}\in\mathbb{R}^n" target="_blank"><img src="http://latex.codecogs.com/gif.latex?v^{(1)}\in\mathbb{R}^n" title="v^{(1)}\in\mathbb{R}^n" /></a> para A(1,2:n)
tal que:

<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\hat{A}&space;&=&space;(I-\sigma_{1,1}u^{(1)}{u^{(1)}}^T)\\&space;&=&space;H_1AG_1&space;=\begin{bmatrix}&space;\alpha_1&space;&&space;\beta_1&&space;0&&space;\cdots&&space;0\\&space;0&&space;x&&space;x&&space;\cdots&&space;x\\&space;\vdots&&space;x&&space;x&&space;\cdots&&space;x\\&space;0&&space;x&&space;x&&space;\cdots&&space;x&space;\end{bmatrix}&space;\end{align*}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{align*}&space;\hat{A}&space;&=&space;(I-\sigma_{1,1}u^{(1)}{u^{(1)}}^T)\\&space;&=&space;H_1AG_1&space;=\begin{bmatrix}&space;\alpha_1&space;&&space;\beta_1&&space;0&&space;\cdots&&space;0\\&space;0&&space;x&&space;x&&space;\cdots&&space;x\\&space;\vdots&&space;x&&space;x&&space;\cdots&&space;x\\&space;0&&space;x&&space;x&&space;\cdots&&space;x&space;\end{bmatrix}&space;\end{align*}" title="\begin{align*} \hat{A} &= (I-\sigma_{1,1}u^{(1)}{u^{(1)}}^T)\\ &= H_1AG_1 =\begin{bmatrix} \alpha_1 & \beta_1& 0& \cdots& 0\\ 0& x& x& \cdots& x\\ \vdots& x& x& \cdots& x\\ 0& x& x& \cdots& x \end{bmatrix} \end{align*}" /></a>

De esta manera eliminamos la primer columna y la primera fila y debemos realizar más iteraciones para obtener la matriz bidiagonal. Las matrices izquierdas <a href="http://www.codecogs.com/eqnedit.php?latex=H_i's" target="_blank"><img src="http://latex.codecogs.com/gif.latex?H_i's" title="H_i's" /></a> y las derechas <a href="http://www.codecogs.com/eqnedit.php?latex=G_i's" target="_blank"><img src="http://latex.codecogs.com/gif.latex?G_i's" title="G_i's" /></a> son llamadas matrices Householder, la eliminación siguiente se obtiene:

<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;B&space;&=&space;Q^TAP&space;\end{align*}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{align*}&space;B&space;&=&space;Q^TAP&space;\end{align*}" title="\begin{align*} B &= Q^TAP \end{align*}" /></a>

donde,

<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;Q^t&space;&=&space;\prod_{i=1}^nH_iP=\prod_{i=1}^{n-2}G_i&space;\end{align*}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{align*}&space;Q^t&space;&=&space;\prod_{i=1}^nH_iP=\prod_{i=1}^{n-2}G_i&space;\end{align*}" title="\begin{align*} Q^t &= \prod_{i=1}^nH_iP=\prod_{i=1}^{n-2}G_i \end{align*}" /></a>

aquí <a href="http://www.codecogs.com/eqnedit.php?latex=H_i=I-\sigma_{1,i}u^{(i)}{u^{(i)}}^t" target="_blank"><img src="http://latex.codecogs.com/gif.latex?H_i=I-\sigma_{1,i}u^{(i)}{u^{(i)}}^t" title="H_i=I-\sigma_{1,i}u^{(i)}{u^{(i)}}^t" /></a> y <a href="http://www.codecogs.com/eqnedit.php?latex=G_i=I-\sigma_{2,i}v^{(i)}{v^{(i)}}^t" target="_blank"><img src="http://latex.codecogs.com/gif.latex?G_i=I-\sigma_{2,i}v^{(i)}{v^{(i)}}^t" title="G_i=I-\sigma_{2,i}v^{(i)}{v^{(i)}}^t" /></a>, los vectores u y v son una elección de vectores householder para la transformación. En general para un vector <a href="http://www.codecogs.com/eqnedit.php?latex=y&space;\in&space;\mathbb{R}^l" target="_blank"><img src="http://latex.codecogs.com/gif.latex?y&space;\in&space;\mathbb{R}^l" title="y \in \mathbb{R}^l" /></a> la selección de vectores householder **r** y escalares <a href="http://www.codecogs.com/eqnedit.php?latex=\sigma" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\sigma" title="\sigma" /></a> y <a href="http://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a> está dado por lo siguiente:

<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;\alpha&space;&=&space;-signo(y_1)||y||,&space;a=sign(y_1)||y||\\&space;\sigma&space;&=&space;(y_1&plus;a)/a\\&space;\mathbf{r}&space;&=&space;\frac{y&plus;[a,0,\cdots,0]^T}{y_1&plus;a}&space;\end{align*}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{align*}&space;\alpha&space;&=&space;-signo(y_1)||y||,&space;a=sign(y_1)||y||\\&space;\sigma&space;&=&space;(y_1&plus;a)/a\\&space;\mathbf{r}&space;&=&space;\frac{y&plus;[a,0,\cdots,0]^T}{y_1&plus;a}&space;\end{align*}" title="\begin{align*} \alpha &= -signo(y_1)||y||, a=sign(y_1)||y||\\ \sigma &= (y_1+a)/a\\ \mathbf{r} &= \frac{y+[a,0,\cdots,0]^T}{y_1+a} \end{align*}" /></a>

tal que <a href="http://www.codecogs.com/eqnedit.php?latex=(I-\sigma\mathbf{rr}^T)y=[\alpha,0,...,0]^T" target="_blank"><img src="http://latex.codecogs.com/gif.latex?(I-\sigma\mathbf{rr}^T)y=[\alpha,0,...,0]^T" title="(I-\sigma\mathbf{rr}^T)y=[\alpha,0,...,0]^T" /></a>. En nuestro caso tenemos que los vectores u y v serán calculados como **r** y los escalares <a href="http://www.codecogs.com/eqnedit.php?latex=\alpha_i" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\alpha_i" title="\alpha_i" /></a> y <a href="http://www.codecogs.com/eqnedit.php?latex=\beta_i" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\beta_i" title="\beta_i" /></a> serán calculados como <a href="http://www.codecogs.com/eqnedit.php?latex=\alpha" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\alpha" title="\alpha" /></a>.

De esta manera obtenemos los vectores para realizar las transformaciones y poder realizar con éxito las eliminaciones de cada renglón-columna (hacerlos ceros). Podemos entonces resumir la actualización de la matriz A después de la i-ésima eliminación renglón-columna de la siguiente manera:

<a href="http://www.codecogs.com/eqnedit.php?latex=A(i&plus;1:m,i&plus;1:n)=A(i&plus;1:m,i&plus;1:n)-\hat{\mathbf{u}}^{(i)}{\hat{\mathbf{z}}^{(i)}}^T-\hat{\mathbf{w}}^{(i)}{\hat{\mathbf{z}}^{(i)}}^T" target="_blank"><img src="http://latex.codecogs.com/gif.latex?A(i&plus;1:m,i&plus;1:n)=A(i&plus;1:m,i&plus;1:n)-\hat{\mathbf{u}}^{(i)}{\hat{\mathbf{z}}^{(i)}}^T-\hat{\mathbf{w}}^{(i)}{\hat{\mathbf{z}}^{(i)}}^T" title="A(i+1:m,i+1:n)=A(i+1:m,i+1:n)-\hat{\mathbf{u}}^{(i)}{\hat{\mathbf{z}}^{(i)}}^T-\hat{\mathbf{w}}^{(i)}{\hat{\mathbf{z}}^{(i)}}^T" /></a>

donde, 

<a href="http://www.codecogs.com/eqnedit.php?latex=\begin{align*}&space;(\hat{\mathbf{z}}^{(i)})^T&space;&=&space;\mathbf{x}^T-\sigma_{2,i}(\mathbf{x}^T\hat{\mathbf{v}}^{(i)})(\hat{\mathbf{v}}^{(i)})^T\\&space;\hat{\mathbf{w}}^{(i)}&=&space;\sigma_{2,i}A(i:m,i&plus;1:n)\hat{\mathbf{v}}^{(i)}\\&space;\mathbf{x}&=&space;\sigma_{1,i}A^T(i:m,i&plus;1:n)\hat{\mathbf{u}}^{(i)}&space;\end{align*}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\begin{align*}&space;(\hat{\mathbf{z}}^{(i)})^T&space;&=&space;\mathbf{x}^T-\sigma_{2,i}(\mathbf{x}^T\hat{\mathbf{v}}^{(i)})(\hat{\mathbf{v}}^{(i)})^T\\&space;\hat{\mathbf{w}}^{(i)}&=&space;\sigma_{2,i}A(i:m,i&plus;1:n)\hat{\mathbf{v}}^{(i)}\\&space;\mathbf{x}&=&space;\sigma_{1,i}A^T(i:m,i&plus;1:n)\hat{\mathbf{u}}^{(i)}&space;\end{align*}" title="\begin{align*} (\hat{\mathbf{z}}^{(i)})^T &= \mathbf{x}^T-\sigma_{2,i}(\mathbf{x}^T\hat{\mathbf{v}}^{(i)})(\hat{\mathbf{v}}^{(i)})^T\\ \hat{\mathbf{w}}^{(i)}&= \sigma_{2,i}A(i:m,i+1:n)\hat{\mathbf{v}}^{(i)}\\ \mathbf{x}&= \sigma_{1,i}A^T(i:m,i+1:n)\hat{\mathbf{u}}^{(i)} \end{align*}" /></a>

Las matrices <a href="http://www.codecogs.com/eqnedit.php?latex=P" target="_blank"><img src="http://latex.codecogs.com/gif.latex?P" title="P" /></a> y <a href="http://www.codecogs.com/eqnedit.php?latex=Q" target="_blank"><img src="http://latex.codecogs.com/gif.latex?Q" title="Q" /></a> de la bidiagonalización <a href="http://www.codecogs.com/eqnedit.php?latex=B=Q^TAP" target="_blank"><img src="http://latex.codecogs.com/gif.latex?B=Q^TAP" title="B=Q^TAP" /></a> son obtenidas de una forma similar. PAra obtener la matriz <a href="http://www.codecogs.com/eqnedit.php?latex=B" target="_blank"><img src="http://latex.codecogs.com/gif.latex?B" title="B" /></a> se utilizó bidiagonalización parcial debido a que es menos costoso y fue previamente implementado en GPU.


Una vez descrita la bidiagonalización se menciona que las actualizaciones se pueden expresar utilizando operaciones de nivel 2 de BLAS, actualizando la matriz cada que ocurre una eliminación renglón-columna. Dado que el método es computacionalmente costoso se propone dividir la matriz en bloques y a cada bloque aplicar la bidiagonalización, si dividimos a la matriz en L bloques entonces la actualización completa de la matriz se dará cuando L columnas y renglones sean bidiagonalizados. Dado que cada bloque sufre una actualización se requieren cálculos extra de los vectores u,v,w y z. Dado que cada bloque tendrá sus vectores householder es necesario almacenar en memoria los cada uno en una matriz U,V,W y Z respectivamente ya que estos serán necesarios para la actualización de la matriz completa.

El algortimo descrito es implementado en GPU mediante CUBLAS, el enfoque por bloques tiene un desempeño alto debido a que las funciones de multiplicación y normas que tiene CUBLAS para matriz-matriz y matriz-vector. Se menciona que CUBLAS se desempeña mejor para matrices cuyo tamaño es un múltiplo de 32 (debido a que se alineaba con la capacidad de memoria) y se aprovechó este hecho al agregar ceros a las matrices y vectores para que su longitud fuer aun múltiplo de 32.

Algo que es importante mencionar es que el desempeño de la GPU también depende de en donde se encuentren alojados los datos. Se asume que inicialmente los datos(la matriz) se encuentran en la CPU y son transferidos a la GPU. Además dado que el bando de ancha entre la CPU y GPU es de magnitud menor al de sólo GPU se propone inicializar las matrices invloucradas en la bidiagonalización en ls GPU. Una vez que se ha bidiagonalizado la matriz A esta es copiada a la CPU para proceder con la diagonalización y se mantienen la matrices <a href="http://www.codecogs.com/eqnedit.php?latex=Q" target="_blank"><img src="http://latex.codecogs.com/gif.latex?Q" title="Q" /></a> y <a href="http://www.codecogs.com/eqnedit.php?latex=P^T" target="_blank"><img src="http://latex.codecogs.com/gif.latex?P^T" title="P^T" /></a> en la GPU.






